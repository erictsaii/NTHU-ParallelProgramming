# 113062526 蔡芝泰 HW2 Report
## Implementation 
### Pthread version
* 首先透過`sched_getaffinity`來得到可用的CPU數量，初始化完一些變數之後就透過`pthread_create`創建相對應數量的thread來計算mandelbrot set。
* 為了達成比較好的load balance，我設了一個全域變數來記錄當前計算到第幾個width，並且用`pthread_mutex_lock`, `pthread_mutex_unlock`保護起來，防止race condiiton的問題。
```cpp=
pthread_mutex_t mutex;
int cur_width;
void* mandelbrot_col(void* threadid) {
while (true) {
    int w = 0;
    bool last_one = false;
    pthread_mutex_lock(&mutex);
    w = cur_width;
    cur_width += 8;
    pthread_mutex_unlock(&mutex);
    if (w >= width) break;
    // continue...
}
```
* 接下來就是計算的部分，這邊使用了avx512來進行加速mandelbrot set的計算，加速效果非常明顯，因為這樣一次就可以做8個double的計算，這邊還有運用到一個比較特別的東西叫做mask，可以用bitwise的方式去控制某個double要不要做運算，之前我檢查`length_squared`的方式是一個個去檢查，會多一個for loop，讓整個計算時間多了不少，後來改成用`_mm512_cmplt_pd_mask`去檢查，之後再用`_mm256_mask_add_epi32`對還未完成計算的repeat做+1的動作。
* 最後再使用`_mm256_mask_storeu_epi32`將結果存回`image`，這邊的mask就是為了要處理餘數的部分，因為width不一定會是8的倍數。
```cpp=
while(true){
    // omit the part we've mentioned
    for (int i = 0; i < height; ++i) {
        // omit initialize...

        // calculate
        for (int j = 0; j < iters; ++j) {
            xx = _mm512_mul_pd(x, x);
            yy = _mm512_mul_pd(y, y);
            length_squared = _mm512_fmadd_pd(x, x, yy);

            finished_mask = _mm512_cmplt_pd_mask(length_squared, four);
            finished_mask &= mask;
            if (finished_mask == 0) break;
            repeats = _mm256_mask_add_epi32(repeats, finished_mask, repeats, one);

            xy = _mm512_mul_pd(x, y);
            y = _mm512_fmadd_pd(xy, two, y0);
            x = _mm512_add_pd(_mm512_sub_pd(xx, yy), x0);
        }
        _mm256_mask_storeu_epi32((__m256*)&image[w + width * i], mask, repeats);
    }
}
```
### Hybrid Version
* hybrid版本是用MPI + OpenMP來進行加速，我分配task的方法是把height分給process，每個process再用omp去對於for loop去做平行。
* 因為每個task執行時間差異比較大的關係，我使用的schedule是dynamic，這樣可以有比較好的load balance，而計算的部分跟pthread的版本一樣都會使用avx512來進行加速。都計算完之後，最後再使用`MPI_Reduce`將答案收集起來進行輸出。
```cpp=
#pragma omp parallel for schedule(dynamic)
for (int h = rank; h < height; h += size) {
    for (int w = 0; w < width; w += 8) {  
        // omit initialize...

        // calculate
        for (int j = 0; j < iters; j++) {
            xx = _mm512_mul_pd(x, x);
            yy = _mm512_mul_pd(y, y);
            length_squared = _mm512_fmadd_pd(x, x, yy);

            __mmask8 finished_mask = _mm512_cmplt_pd_mask(length_squared, four);
            finished_mask &= mask;
            if (finished_mask == 0) break;
            repeats = _mm256_mask_add_epi32(repeats, finished_mask, repeats, one);

            xy = _mm512_mul_pd(x, y);
            y = _mm512_fmadd_pd(xy, two, y0);
            x = _mm512_add_pd(_mm512_sub_pd(xx, yy), x0);
        }
        _mm256_mask_storeu_epi32((__m256*)&image[w + width * h], mask, repeats);
    }
}
MPI_Reduce(image, final_image, width * height, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);
```
### Other Optimization
* write_png()的優化: filter的部分可以改成PNG_FILTER_NONE，而compression level可以改成0以換取更快的速度，優化的效果很明顯。
```cpp=
png_set_filter(png_ptr, 0, PNG_FILTER_NONE);
png_set_compression_level(png_ptr, 0);
```
## Experiment & Analysis
### i、Methodology
#### (a). System Spec
* 我使用的是學校提供的qct server。
#### (b). Performance Metrics
* 我使用Nsight作為profiler，並使用nvtx去測量`write_png()`的執行時間。
* `IO time`代表的是`write_png()`的執行時間。
* `Communication time`代表的是`MPI_Reduce`的執行時間。
* `Compute time`是透過總執行時間減掉`IO time`所得。
### ii、Plots: Scalability & Load Balancing & Profile
#### Experimental Method:
* Test Case Description: 我使用的是slow10，執行時間夠長，能夠看出在各種環境配置下的差異。
* Parallel Configurations: 在pthread的實驗中，我嘗試了1, 4, 8, 12個thread;而在hybrid的實驗中，則是先嘗試了single process，用了1, 4, 8, 12個thread，之後又用了4個process，使用了4, 12, 24, 48個thread。
#### Performance Measurement:
* pthread (unit: second)
  | # of thread | I/O    | compute |
  |:----------- |:------ |:------- |
  | 1           | 0.0363 | 9.374   |
  | 4           | 0.0377 | 2.363   |
  | 8           | 0.0373 | 1.243   |
  | 12          | 0.0377 | 0.862   |
* hybrid: single process (unit: second)
  | # of thread | I/O    | compute | communication |
  |:----------- |:------ |:------- |:------------- |
  | 1           | 0.0366 | 9.46    | 0.006         |
  | 4           | 0.0362 | 2.55    | 0.008         |
  | 8           | 0.0377 | 1.38    | 0.0058        |
  | 12          | 0.0377 | 1.01    | 0.0054        |
* hybrid: four processes (unit: second)
  | # of thread | I/O    | compute | communication |
  |:----------- |:------ |:------- |:------------- |
  | 4           | 0.033  | 2.69    | 0.037         |
  | 12          | 0.0326 | 1.14    | 0.029         |
  | 24          | 0.034  | 0.64    | 0.0285        |
  | 48          | 0.0327 | 0.59    | 0.0253        |


#### Analysis of Results:
##### pthread
![image](https://hackmd.io/_uploads/HkvgWAXbJe.png)
![image](https://hackmd.io/_uploads/rkt5mCmbke.png)
* 以speedup factor來說，pthread版本的優化效果已經很不錯了，在thread數量為4和8時已經達到接近linear的加速，這也可能是因為這支程式主要都是compute time，I/O time的影響微乎其微，而計算行為也很簡單，沒有多餘的溝通，所以加速效果才會這麼好。

##### hybrid:single process
![image](https://hackmd.io/_uploads/B1EwMAmb1x.png)
![image](https://hackmd.io/_uploads/H1ZvBAQb1l.png)
* 以single process來說，其實比較的就是omp的平行效果，雖然對比pthread還是稍微遜色，但光是用compiler的方式就可以有這種效果還是很不錯的。
##### hybrid:four processes
![image](https://hackmd.io/_uploads/HJeHEAQZye.png)
![image](https://hackmd.io/_uploads/HytMHCXZkx.png)
* 這邊用了四個process來做實驗，而thread的數量就是平均分配給這些process，可以發現communication time跟I/O都只佔了一點點，不過因為總執行時間已經變得很短，speedup factor就不像前面這麼好，尤其在48個thread的時候的優化幅度就很小了，推測是有些計算上的overhead開始蓋過了平行的優化效果。
##### load balance
![image](https://hackmd.io/_uploads/SyEnHk4byg.png)
![image](https://hackmd.io/_uploads/SkDHVk4-ye.png)
* 由於我都是讓第一個thread/process去做I/O的關係，所以他們的執行時間會稍微長一點，而從圖表可以看出其實是很load balance的，其餘的每個thread/process的執行時間都相當接近。


#### Optimization Strategies:
* 由於這次作業基本上都是compute bound，若想要再更加優化的話可能可以在對於餘數的處理上更加fine grained，或是在分配task的時候做到以pixel為單位去分配，這樣可能會有更極致的load balance。

### iii、Discussion
* scalability的部分在pthread版本已經相當不錯，有接近linear speedup的優化效果，而hybrid的話由於omp對比pthread還是有一點點效能上的落差，所以scalability相較pthread是稍微差一點，但跟作業一相比，此次作業的scalability是好很多的，因為I/O跟communication的時間跟compute相比是非常短的。
* load balance的部分透過前面的圖表可以看出表現還不錯，除了第一個thread/process因為要寫檔案的關係，執行時間稍微長了一點點，不過其餘thread/process的執行時間都很平均，有達到load balance。
## Experience & Conclusion
* 這次作業學會使用avx512來進行加速，加速的效果非常好，也發現omp的平行效果雖然稍微落後pthread，不過omp光是用compiler的方式就可以達到這種效果還是很不錯的。在debug的時候比較常遇到accuracy的問題，有時候是沒考慮好餘數的問題，或是fmadd的輸出結果精度會比較低，花了比較多時間在處理類似的bug。
